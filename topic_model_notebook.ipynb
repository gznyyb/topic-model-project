{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the nltk package and download the stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import other useful language processing tools\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# import packages for visualization\n",
    "!pip install seaborn==0.9.0\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# import other useful packages \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "poetry_dataset = pd.read_csv('poetry.csv')\n",
    "poetry_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_extend(extension_list, extension_file):\n",
    "    '''Function to extend the default nltk stopwords list\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    extension_list: a list of words to extend the list of stop words\n",
    "    extension_file: a .txt file that contains a list of words to extend the list of stop words\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    en_stop_words: the final list of stop words\n",
    "    '''\n",
    "    en_stop_words = stopwords.words('english')\n",
    "    en_stop_words.extend(extension_list)\n",
    "    more_stopwords = open(extension_file).read().split()\n",
    "    en_stop_words.extend(more_stopwords)\n",
    "    en_stop_words = set(en_stop_words)\n",
    "    return en_stop_words\n",
    "\n",
    "# the following domain specific stopwords are added to the stopwords list \n",
    "extension_list = ['thy', 'thou', 'let', 'thee', 'thine', 'thyself', 'tis', 'doth',\n",
    "                  'upon', 'till', 'unto', 'hath', 'ye', '-PRON-', 'shalt']\n",
    "\n",
    "# set up and print out the resulting stopwords list\n",
    "stopword_list = set(stop_word_extend(extension_list, 'stopwords_extend.txt'))\n",
    "\n",
    "print(\"the total number of stopwords: {}\".format(len(stopword_list)))\n",
    "list(stopword_list)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text, stop_word_extension_list, extension_list):\n",
    "    '''Function to preprocess each poetry of all the poetires listed in the content column of the dataset\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text: a string represents individual poetry\n",
    "    stop_word_extension_list: a list of words to extend the list of stop words\n",
    "    extension_list: a .txt file that contains a list of words to extend the list of stop words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text: preprocessed text of individual poetry\n",
    "    '''\n",
    "    text = text.lower().strip() # strip spaces before and after each line of texts\n",
    "    # extract the lemma of words so that words can be reduced to their basic forms\n",
    "    text = \" \".join([token.lemma_ for token in nlp(text)]) \n",
    "    text = \" \".join([word for word in simple_preprocess(text, deacc=True)]) # remove punctuations\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_word_extend(stop_word_extension_list, \n",
    "      extension_list)]) # remove stopwords\n",
    "    text = re.sub(r'\\s*lov\\s', 'love', text) \n",
    "    text = strip_short(text) # remove words that are too short\n",
    "    return text\n",
    "\n",
    "# function to preprocess all the poetries and get rid of problematic rows that \n",
    "# contain copyright information \n",
    "def get_text_prepared(poetry_dataset, stop_word_extension_file, extension_list):\n",
    "    '''Function to preprocess all the poetries and get rid of problematic rows that contain copyright information\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    poetry_dataset: the .csv file that is the poetry dataset\n",
    "    stop_word_extension_file: a .txt file that contains a list of words to extend the list of stop words\n",
    "    extension_list: a list of words to extend the list of stop words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text_data: a list containing list of words from each poetry after the preprocessing\n",
    "    poetry_dataset_reduced: a reduced version of the poetry dataset after the rows containing the copyright information have been deleted\n",
    "    '''\n",
    "    text_data = []\n",
    "    good_index = []\n",
    "    for i, poetry in enumerate(poetry_dataset['content']):\n",
    "        if any(x in ['copyright', 'permission', 'published'] for x in poetry.lower().split()):\n",
    "            continue\n",
    "        text_data.append(text_prepare(poetry, extension_list, \n",
    "          stop_word_extension_file).split())\n",
    "        good_index.append(i)\n",
    "    poetry_dataset_reduced = poetry_dataset.iloc[good_index, :]\n",
    "    return text_data, poetry_dataset_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all poetries\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "text_data, poetry_dataset_reduced = get_text_prepared(poetry_dataset,\n",
    "                                                      'stopwords_extend.txt',\n",
    "                                                      extension_list)\n",
    "\n",
    "# print out a comparison between the preprocessed and raw text \n",
    "pprint.pprint(' '.join(text_data[0]))\n",
    "print(poetry_dataset['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(text_data, min_count, threshold):\n",
    "    '''Function to combine words together to form phrases so that the modeling afterwards can learn some context.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text_data: list of documents each of which is a list of words\n",
    "    min_count:\n",
    "    threshold:\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text_data: list of documents each of which is a list of words and trigrams\n",
    "    '''\n",
    "    bigram = Phrases(text_data, min_count=min_count, threshold=threshold)\n",
    "    trigram = Phrases(bigram[text_data], threshold=threshold)\n",
    "\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "\n",
    "    def make_bi_tri_grams(text_data):\n",
    "        text_data = [bigram_mod[doc] for doc in text_data]\n",
    "        text_data = [trigram_mod[doc] for doc in text_data]\n",
    "        return text_data\n",
    "\n",
    "    text_data = make_bi_tri_grams(text_data=text_data)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gensim_corpus(text_data):\n",
    "    '''Function to get the dictionary and corpus for modeling. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text_data: list of documents each of which is a list of words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    corpus: list of documents each of which consists of tuple of word in their integer representation and their counts in the document\n",
    "    dictionary: the mapping between words and their integer ids\n",
    "    '''\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_grams = make_ngrams(text_data, 5, 70)\n",
    "corpus, dictionary = get_gensim_corpus(text_data_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_search_range = np.arange(2, 8, 1) # the number of topics to try\n",
    "coherence_scores = [] # list to store the coherence scores\n",
    "models = [] # list to store the LDA models \n",
    "\n",
    "# loop to fit LDA models of different number of topics\n",
    "for num_loops, n_topics in enumerate(n_topics_search_range):\n",
    "    print(\"training LDA model with %d topics\" % (n_topics))\n",
    "    # fit the model\n",
    "    topic_model_LDA = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                               num_topics=n_topics, random_state=10,\n",
    "                               update_every=1, chunksize=80,\n",
    "                               passes=20, alpha='auto',\n",
    "                               per_word_topics=True)\n",
    "    \n",
    "    # compute the coherence scores \n",
    "    coherence_score_LDA = CoherenceModel(model=topic_model_LDA, \n",
    "                                         texts=text_data, coherence='c_v', \n",
    "                                         dictionary=dictionary).get_coherence()\n",
    "    \n",
    "    # store the coherence scores and models \n",
    "    coherence_scores.append(coherence_score_LDA)\n",
    "    models.append(topic_model_LDA)\n",
    "\n",
    "# convert the coherence scores into a dataframe for plotting \n",
    "coherence_scores_data = pd.DataFrame(coherence_scores, index=n_topics_search_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 9))\n",
    "sns.set_style('white')\n",
    "sns.lineplot(data=coherence_scores_data)\n",
    "plt.xlabel(\"the number topics trained\")\n",
    "plt.ylabel(\"coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_used = models[1][1]\n",
    "pprint.pprint(model_used.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(model_used, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization contains the following elements:\n",
    "* The bubbles on the left shows the marginal distribution of each topic. This is related to the percentage of tokens it contains. \n",
    "* The further away a bubble is from from another bubble the more different between their meanings. Therefore, from the above visualization we can see that topic 1 and 2 have similar meanings. \n",
    "* From the bar graph on the right we can see the information about each word. The red bar indicates the frequency of the token within the topic and the light blue overall frequency within the corpus. \n",
    "* Furthermore, we can adjust the relevance metric to get more information about a certain topic by ranking the tokens not only by its frequency within the topic but ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = []\n",
    "doc_dist_list = []\n",
    "for document in corpus:\n",
    "    doc_dist = model_used.get_document_topics(document, minimum_probability=0)\n",
    "    doc_dist_list.append(list(map(lambda x: x[1], doc_dist)))\n",
    "    doc_dist_sorted = sorted(doc_dist, key=lambda x: x[1], reverse=True)\n",
    "    doc_topic.append(doc_dist_sorted[0][0])\n",
    "    \n",
    "poetry_dataset_reduced['topics'] = doc_topic\n",
    "\n",
    "tsne_result = TSNE(random_state=10).fit_transform(doc_dist_list)\n",
    "tsne_result = pd.DataFrame(tsne_result, columns=['coordinate1', 'coordinate2'])\n",
    "\n",
    "poetry_dataset_reduced.reset_index(inplace=True, drop=True)\n",
    "poetry_dataset_prepared = pd.concat([poetry_dataset_reduced, tsne_result],\n",
    "                                    axis=1)\n",
    "poetry_dataset_prepared.astype({'topics': 'category'})\n",
    "\n",
    "author_list = ['WILLIAM SHAKESPEARE', 'EDMUND SPENSER', 'WILLIAM BUTLER YEATS',\n",
    "              'D. H. LAWRENCE', 'JOHN DONNE', 'WALLACE STEVENS']\n",
    "author_mask = poetry_dataset_prepared.author.isin(author_list)\n",
    "poetry_dataset_prepared_reduced = poetry_dataset_prepared.loc[author_mask,:]\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.countplot(x='topics', hue='age', data=poetry_dataset_prepared)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "num_topics = len(set(poetry_dataset_reduced['topics']))\n",
    "sns.scatterplot(x='coordinate1', y='coordinate2', style='author',\n",
    "                hue='topics', data=poetry_dataset_prepared_reduced,\n",
    "               palette=sns.color_palette(\"husl\", num_topics))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
